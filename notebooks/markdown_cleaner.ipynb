{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ab6935",
   "metadata": {},
   "source": [
    "# Markdown Cleaner å·¥ä½œæµç¨‹\n",
    "\n",
    "é€™ä»½ Notebook æœƒå¾å–®ä¸€ Markdown ä¾†æºç”¢ç”Ÿï¼š\n",
    "- ï¼ˆå¯é¸ï¼‰å¥—ç”¨ç§»é™¤ç²—é«”ã€è¨»è§£æˆ–çµ±ä¸€æ¨™é¡Œç­‰æ–‡å­—è™•ç†\n",
    "- ï¼ˆå¯é¸ï¼‰è¼¸å‡ºæ•´ç†å¾Œçš„ Markdown æª”æ¡ˆ\n",
    "- è—‰ç”± Pandoc ç”¢ç”Ÿ Word æª”ï¼Œå†ç”¨ python-docx åšå¾Œè£½ï¼ˆç›®éŒ„ã€åˆ†é ã€å¥—ç”¨ç¯„æœ¬ï¼‰\n",
    "\n",
    "> ğŸ’¡ Notebook å¹³å¸¸åœ¨ Docker å®¹å™¨å…§åŸ·è¡Œï¼šå®¹å™¨è² è²¬è½‰æª”ï¼Œä½†è‹¥è¦åœ¨ Word ä¸­æ›´æ–°æ¬„ä½ï¼ˆå±•é–‹è‡ªå‹•ç›®éŒ„ï¼‰ï¼Œå¿…é ˆåœ¨ä¸»æ©Ÿä¸Šé¡å¤–åŸ·è¡Œè…³æœ¬ã€‚ä¸‹æ–¹æœ‰ç›¸é—œèªªæ˜ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e8e0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ---- è¨­å®šå€ï¼šå¯ä¾éœ€æ±‚èª¿æ•´ ----\n",
    "\n",
    "source_relative = Path('mkdocs/My_Notes/1017é»ƒèŒ‚æ¦®/æŸ¯è€å¸«1017è¬›ç¨¿.md')\n",
    "\n",
    "# source_relative = Path('mkdocs/My_Notes/113æ†²åˆ¤å­—11è™Ÿåˆ¤æ±º/æ³•å®˜å­¸é™¢æ›¸é¢ç¨¿.md')\n",
    "\n",
    "# è‹¥éœ€è¦è¦†å¯«è¼¸å‡ºè·¯å¾‘ï¼Œå¯å–æ¶ˆè¨»è§£ä¸¦è¨­å®š\n",
    "# output_relative = Path('mkdocs/My_Notes/113æ†²åˆ¤å­—11è™Ÿåˆ¤æ±º/æ³•å®˜å­¸é™¢æ›¸é¢ç¨¿_ç„¡ç²—é«”.md')\n",
    "# word_output_relative = Path('mkdocs/My_Notes/113æ†²åˆ¤å­—11è™Ÿåˆ¤æ±º/çµ¦è€å¸«çš„/éºç”¢ç¨…æ¡ˆä¾‹ç ”è¨_æ³•å®˜å­¸é™¢æ›¸é¢ç¨¿_20251016.docx')\n",
    "\n",
    "output_relative = None\n",
    "word_output_relative = None\n",
    "word_template_relative = Path('mkdocs/My_Notes/wordæ¨£å¼æ–‡ä»¶å¤¾/æˆ‘çš„wordæ¨£å¼æª”æ¡ˆ.dotx')\n",
    "toc_marker = r'\\tableofcontents'  # åœ¨åŸ Markdown æŒ‡å®šç›®éŒ„ä½ç½®çš„æ¨™è¨˜\n",
    "\n",
    "# åŒ¯å‡ºé–‹é—œï¼šé è¨­åƒ…è¼¸å‡º Wordï¼Œå¯è¦–éœ€æ±‚èª¿æ•´ã€‚\n",
    "export_markdown = False\n",
    "export_word = True\n",
    "insert_page_breaks_for_h2 = False  # æ˜¯å¦æ–¼æ¯å€‹äºŒç´šæ¨™é¡Œå‰æ›é \n",
    "\n",
    "\n",
    "remove_bold = False\n",
    "remove_comments = True\n",
    "normalize_headings = False\n",
    "\n",
    "# è‡ªå‹•è¼¸å‡ºå‘½åè¨­å®šï¼šæœªæä¾›è‡ªè¨‚è·¯å¾‘æ™‚ï¼Œä»¥ä¸‹è¦å‰‡å°‡è‡ªå‹•ç”¢ç”Ÿæª”åèˆ‡è³‡æ–™å¤¾ã€‚\n",
    "auto_markdown_suffix = '_ç„¡ç²—é«”'\n",
    "auto_word_suffix = ''\n",
    "auto_word_folder_name = None  # ä¾‹: 'çµ¦è€å¸«çš„'\n",
    "auto_filtered_prefix = 'Filtered_'\n",
    "\n",
    "# --------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db2a20",
   "metadata": {},
   "source": [
    "## è¨­å®šå€ä¸€è¦½\n",
    "\n",
    "- `source_relative`ï¼šä¾†æº Markdown çš„ç›¸å°è·¯å¾‘ã€‚\n",
    "- `output_relative` / `word_output_relative`ï¼šå¦‚çœç•¥ï¼Œæœƒä¾å‘½åè¦å‰‡è‡ªå‹•ç”¢ç”Ÿå°æ‡‰è·¯å¾‘ã€‚\n",
    "- `word_template_relative`ï¼šWord ç¯„æœ¬ï¼Œå¯æ²¿ç”¨æ¨£å¼ï¼ˆä¾‹å¦‚ç›®éŒ„å­—å‹ï¼‰ã€‚\n",
    "- `export_markdown` / `export_word`ï¼šæ§åˆ¶æ˜¯å¦è¼¸å‡ºå°æ‡‰æ ¼å¼ã€‚\n",
    "- `insert_page_breaks_for_h2`ï¼šæ˜¯å¦åœ¨æ¯å€‹ `##` å‰æ’å…¥æ›é ï¼Œé è¨­é—œé–‰ã€‚\n",
    "- `toc_marker`ï¼šåœ¨ Markdown ä¸­æ”¾ç½®çš„ç›®éŒ„æ¨™è¨˜ï¼ˆé è¨­ `\\tableofcontents`ï¼‰ã€‚Notebook æœƒæŠŠå®ƒæ›æˆ placeholderï¼Œå¾Œè£½æ™‚æ’å…¥çœŸæ­£çš„ç›®éŒ„æ¬„ä½ã€‚\n",
    "- å…¶ä»–å¸ƒæ—è¨­å®šï¼šç§»é™¤ç²—é«”ã€æ¸…é™¤ HTML è¨»è§£ã€çµ±ä¸€æ¨™é¡Œå±¤ç´šã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601e1259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¾†æºæª”æ¡ˆï¼š/home/jovyan/work/mkdocs/My_Notes/1017é»ƒèŒ‚æ¦®/æŸ¯è€å¸«1017è¬›ç¨¿.md\n",
      "åŸå§‹å­—å…ƒæ•¸ï¼š5,048\n",
      "â„¹ï¸ ç•¥é Markdown åŒ¯å‡ºï¼ˆexport_markdown=Falseï¼‰ã€‚\n",
      "å·²è¼¸å‡º Wordï¼ˆpandocï¼‰ï¼š/home/jovyan/work/mkdocs/My_Notes/1017é»ƒèŒ‚æ¦®/æŸ¯è€å¸«1017è¬›ç¨¿.docx\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "try:\n",
    "    from docx import Document\n",
    "    from docx.enum.text import WD_BREAK\n",
    "    from docx.oxml import OxmlElement\n",
    "    from docx.oxml.ns import qn\n",
    "except ImportError:\n",
    "    Document = None\n",
    "    WD_BREAK = None\n",
    "    OxmlElement = None\n",
    "    qn = None\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from zipfile import ZipFile\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "possible_roots = [BASE_DIR, BASE_DIR / \"work\"] + list(BASE_DIR.parents) + list((BASE_DIR / \"work\").parents)\n",
    "\n",
    "NS = {\"w\": \"http://schemas.openxmlformats.org/wordprocessingml/2006/main\"}\n",
    "TOC_PLACEHOLDER = \"[[DOCX_TOC_PLACEHOLDER]]\"\n",
    "\n",
    "\n",
    "def ensure_docx_package():\n",
    "    if any(x is None for x in (Document, WD_BREAK, OxmlElement, qn)):\n",
    "        raise ImportError(\"python-docx æœªå®‰è£ï¼Œè«‹å…ˆåœ¨æ­¤ notebook åŸ·è¡Œ `%pip install python-docx`\")\n",
    "\n",
    "\n",
    "def resolve_path(relative):\n",
    "    p = Path(relative)\n",
    "    if p.is_absolute():\n",
    "        return p if p.exists() else None\n",
    "    for base in possible_roots:\n",
    "        candidate = (Path(base) / p).resolve()\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "\n",
    "def export_to_docx(text: str, destination: Path, reference_doc: Path | None = None) -> None:\n",
    "    import subprocess, tempfile, shutil, re\n",
    "\n",
    "    # æ¸…é™¤å¯èƒ½ç ´å£ Pandoc çš„éš±è—ç¬¦è™Ÿèˆ‡éå°å­—å…ƒï¼ˆç¢ºä¿è¼¸å‡ºåˆæ³•ï¼‰ã€‚\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\u200B\\uFEFF]', '', text)\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    # Pandoc åƒ…èƒ½è™•ç†åˆæ³• UTF-8\n",
    "    text.encode('utf-8', 'strict')\n",
    "\n",
    "    if shutil.which(\"pandoc\") is None:\n",
    "        raise RuntimeError(\"pandoc æœªå®‰è£\")\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(\"w\", suffix=\".md\", encoding=\"utf-8\", delete=False) as tmp:\n",
    "        tmp.write(text)\n",
    "        md = tmp.name\n",
    "\n",
    "    try:\n",
    "        cmd = [\n",
    "            \"pandoc\", md,\n",
    "            \"-f\", \"markdown+pipe_tables+lists_without_preceding_blankline+hard_line_breaks\",\n",
    "            \"-t\", \"docx\",\n",
    "            \"-o\", str(destination),\n",
    "            \"--wrap=none\",\n",
    "        ]\n",
    "        if reference_doc is not None:\n",
    "            cmd.extend([\"--reference-doc\", str(reference_doc)])\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(\"âš ï¸ Pandoc éŒ¯èª¤è¨Šæ¯ï¼š\")\n",
    "            print(result.stderr or result.stdout)\n",
    "            raise subprocess.CalledProcessError(result.returncode, cmd, result.stdout, result.stderr)\n",
    "\n",
    "        print(f\"å·²è¼¸å‡º Wordï¼ˆpandocï¼‰ï¼š{destination}\")\n",
    "    finally:\n",
    "        Path(md).unlink(missing_ok=True)\n",
    "\n",
    "source_path = resolve_path(source_relative)\n",
    "if not source_path:\n",
    "    raise FileNotFoundError(f\"æ‰¾ä¸åˆ°ä¾†æºæª”æ¡ˆï¼š{source_relative}\")\n",
    "\n",
    "repo_root = None\n",
    "for parent in source_path.parents:\n",
    "    candidate = parent / \"mkdocs\"\n",
    "    if candidate.exists():\n",
    "        repo_root = parent\n",
    "        break\n",
    "if repo_root is None:\n",
    "    for base in possible_roots:\n",
    "        base = Path(base)\n",
    "        if (base / \"mkdocs\").exists():\n",
    "            repo_root = base\n",
    "            break\n",
    "if repo_root is None:\n",
    "    repo_root = BASE_DIR\n",
    "\n",
    "output_override = globals().get('output_relative', None)\n",
    "word_output_override = globals().get('word_output_relative', None)\n",
    "word_template_setting = globals().get('word_template_relative', None)\n",
    "export_markdown = globals().get('export_markdown', False)\n",
    "export_word = globals().get('export_word', True)\n",
    "auto_markdown_suffix = globals().get('auto_markdown_suffix', '_cleaned')\n",
    "auto_word_suffix = globals().get('auto_word_suffix', '')\n",
    "auto_word_folder_name = globals().get('auto_word_folder_name', None)\n",
    "auto_filtered_prefix = globals().get('auto_filtered_prefix', 'Filtered_')\n",
    "insert_page_breaks_for_h2 = globals().get('insert_page_breaks_for_h2', False)\n",
    "\n",
    "\n",
    "def resolve_output_path(candidate):\n",
    "    if candidate is None:\n",
    "        return None\n",
    "    candidate = Path(candidate)\n",
    "    if candidate.is_absolute():\n",
    "        resolved = candidate\n",
    "    else:\n",
    "        resolved = (repo_root / candidate).resolve()\n",
    "    resolved.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return resolved\n",
    "\n",
    "\n",
    "def infer_markdown_output_path():\n",
    "    base_dir = source_path.parent\n",
    "    target_dir = base_dir\n",
    "    if auto_filtered_prefix:\n",
    "        if not base_dir.name.startswith(auto_filtered_prefix):\n",
    "            candidate = base_dir.parent / f\"{auto_filtered_prefix}{base_dir.name}\"\n",
    "            if candidate.exists():\n",
    "                target_dir = candidate\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    suffix = auto_markdown_suffix or ''\n",
    "    return target_dir / f\"{source_path.stem}{suffix}{source_path.suffix}\"\n",
    "\n",
    "\n",
    "def infer_word_output_path():\n",
    "    base_dir = source_path.parent\n",
    "    target_dir = base_dir\n",
    "    if auto_word_folder_name:\n",
    "        candidate = base_dir / auto_word_folder_name\n",
    "        if candidate.exists():\n",
    "            target_dir = candidate\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    suffix = auto_word_suffix or ''\n",
    "    return target_dir / f\"{source_path.stem}{suffix}.docx\"\n",
    "\n",
    "\n",
    "def resolve_word_template():\n",
    "    if word_template_setting is None:\n",
    "        return None\n",
    "    candidate = word_template_setting\n",
    "    if not isinstance(candidate, Path):\n",
    "        candidate = Path(candidate)\n",
    "    resolved = resolve_path(candidate)\n",
    "    if resolved is None:\n",
    "        print(f\"âš ï¸ æ‰¾ä¸åˆ° Word æ¨£å¼æª”ï¼š{candidate}\")\n",
    "    return resolved\n",
    "\n",
    "def resolve_style(doc, candidates, fallback='Normal'):\n",
    "    for name in candidates:\n",
    "        try:\n",
    "            return doc.styles[name]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    for style in doc.styles:\n",
    "        style_name = getattr(style, 'name', '')\n",
    "        for name in candidates:\n",
    "            if style_name.endswith(name) or style_name == name:\n",
    "                return style\n",
    "    return doc.styles[fallback]\n",
    "\n",
    "def post_process_docx(docx_path: Path):\n",
    "    \"\"\"ä½¿ç”¨ python-docx åœ¨æŒ‡å®šä½ç½®æ’å…¥ TOC èˆ‡é åˆ†éš”ã€‚\"\"\"\n",
    "    ensure_docx_package()\n",
    "    doc = Document(docx_path)\n",
    "    if not doc.paragraphs:\n",
    "        doc.save(docx_path)\n",
    "        return\n",
    "\n",
    "    # ç§»é™¤ Pandoc é è¨­ç›®éŒ„æ®µè½ï¼ˆå¦‚è‹±æ–‡ Table of Contents æˆ– TOC æ¢ç›®ï¼‰\n",
    "    for para in list(doc.paragraphs):\n",
    "        style_name = para.style.name if para.style else ''\n",
    "        if style_name.startswith('TOC') or 'Table of Contents' in para.text:\n",
    "            p = para._element\n",
    "            p.getparent().remove(p)\n",
    "\n",
    "    apply_page_breaks = globals().get('insert_page_breaks_for_h2', False)\n",
    "\n",
    "    toc_heading_style = resolve_style(doc, ['TOC Heading', '\\u76ee\\u9304', '\\u76ee\\u9304\\u6a19\\u984c'], 'Normal')\n",
    "    toc_entry_style = resolve_style(doc, ['TOC 1', '\\u76ee\\u9304 1'], 'Normal')\n",
    "    toc_heading_text = '\\u76ee\\u9304'\n",
    "\n",
    "    def insert_toc(target_para):\n",
    "        heading_para = target_para.insert_paragraph_before()\n",
    "        heading_para.text = toc_heading_text\n",
    "        heading_para.style = toc_heading_style\n",
    "        field_para = target_para.insert_paragraph_before()\n",
    "        field_para.style = toc_entry_style\n",
    "        run = field_para.add_run()\n",
    "        fld_begin = OxmlElement('w:fldChar')\n",
    "        fld_begin.set(qn('w:fldCharType'), 'begin')\n",
    "        run._r.append(fld_begin)\n",
    "        instr = OxmlElement('w:instrText')\n",
    "        instr.set(qn('xml:space'), 'preserve')\n",
    "        instr.text = r'TOC \\o \"1-4\" \\h \\z \\u'\n",
    "        run._r.append(instr)\n",
    "        fld_sep = OxmlElement('w:fldChar')\n",
    "        fld_sep.set(qn('w:fldCharType'), 'separate')\n",
    "        run._r.append(fld_sep)\n",
    "        fld_end = OxmlElement('w:fldChar')\n",
    "        fld_end.set(qn('w:fldCharType'), 'end')\n",
    "        run._r.append(fld_end)\n",
    "\n",
    "    toc_inserted = False\n",
    "    placeholder_para = None\n",
    "    for para in doc.paragraphs:\n",
    "        if para.text.strip() == TOC_PLACEHOLDER:\n",
    "            placeholder_para = para\n",
    "            break\n",
    "    if placeholder_para is not None:\n",
    "        insert_toc(placeholder_para)\n",
    "        placeholder_para._element.getparent().remove(placeholder_para._element)\n",
    "        toc_inserted = True\n",
    "\n",
    "    if not toc_inserted:\n",
    "        for para in doc.paragraphs:\n",
    "            style_name = para.style.name if para.style else ''\n",
    "            if style_name.startswith('Heading 1'):\n",
    "                insert_toc(para)\n",
    "                toc_inserted = True\n",
    "                break\n",
    "\n",
    "    if not toc_inserted:\n",
    "        insert_toc(doc.paragraphs[0] if doc.paragraphs else doc.add_paragraph())\n",
    "\n",
    "    # åœ¨æ‰€æœ‰ Heading 2 ä¹‹å‰æ’å…¥åˆ†é ï¼ˆè‹¥å‰ä¸€æ®µå·²æ˜¯é åˆ†éš”æˆ–æœªå•Ÿç”¨å‰‡ç•¥éï¼‰\n",
    "    if apply_page_breaks:\n",
    "        for para in doc.paragraphs:\n",
    "            style_name = para.style.name if para.style else ''\n",
    "            if style_name.startswith('Heading 2'):\n",
    "                prev = para._element.getprevious()\n",
    "                page_break_exists = False\n",
    "                if prev is not None:\n",
    "                    for br in prev.findall('.//w:br', namespaces=para._element.nsmap):\n",
    "                        if br.get(qn('w:type')) == 'page':\n",
    "                            page_break_exists = True\n",
    "                            break\n",
    "                if page_break_exists:\n",
    "                    continue\n",
    "                break_para = para.insert_paragraph_before()\n",
    "                break_para.style = doc.styles['Normal']\n",
    "                break_run = break_para.add_run()\n",
    "                break_run.add_break(WD_BREAK.PAGE)\n",
    "\n",
    "    # ç§»é™¤æ®˜ç•™çš„ placeholder æ–‡å­—\n",
    "    for para in list(doc.paragraphs):\n",
    "        if para.text.strip() == TOC_PLACEHOLDER:\n",
    "            p = para._element\n",
    "            p.getparent().remove(p)\n",
    "\n",
    "    doc.save(docx_path)\n",
    "\n",
    "\n",
    "output_path = None\n",
    "if export_markdown:\n",
    "    if output_override is not None:\n",
    "        output_path = resolve_output_path(output_override)\n",
    "    else:\n",
    "        output_path = infer_markdown_output_path()\n",
    "\n",
    "word_output_path = None\n",
    "if export_word:\n",
    "    if word_output_override is not None:\n",
    "        word_output_path = resolve_output_path(word_output_override)\n",
    "    else:\n",
    "        word_output_path = infer_word_output_path()\n",
    "\n",
    "word_template_path = resolve_word_template() if export_word else None\n",
    "\n",
    "text = source_path.read_text(encoding=\"utf-8\")\n",
    "print(f\"ä¾†æºæª”æ¡ˆï¼š{source_path}\")\n",
    "print(f\"åŸå§‹å­—å…ƒæ•¸ï¼š{len(text):,}\")\n",
    "\n",
    "processed = text\n",
    "\n",
    "\n",
    "if remove_bold:\n",
    "    processed = re.sub(r'(?<!\\*)\\*\\*([^\\n*]+)\\*\\*(?!\\*)', r'\\1', processed)\n",
    "\n",
    "\n",
    "if remove_comments:\n",
    "    processed = re.sub(r\"<!--.*?-->\", \"\", processed, flags=re.DOTALL)\n",
    "\n",
    "if normalize_headings:\n",
    "    def normalize(match):\n",
    "        hashes, title = match.group(1), match.group(2).strip()\n",
    "        if len(hashes) <= 1:\n",
    "            return \"# \" + title\n",
    "        return \"## \" + title\n",
    "    processed = re.sub(r\"^(#+)\\s*(.*)$\", normalize, processed, flags=re.MULTILINE)\n",
    "\n",
    "if export_markdown:\n",
    "    if output_path is None:\n",
    "        print('âš ï¸ export_markdown=Trueï¼Œä½†æœªèƒ½æ±ºå®š Markdown è¼¸å‡ºè·¯å¾‘ã€‚')\n",
    "    else:\n",
    "        output_path.write_text(processed, encoding=\"utf-8\")\n",
    "        print(f\"å·²è¼¸å‡º Markdownï¼š{output_path}\")\n",
    "        print(f\"æ–°æª”å­—å…ƒæ•¸ï¼š{len(processed):,}\")\n",
    "else:\n",
    "    if output_override is not None:\n",
    "        print('â„¹ï¸ å·²è¨­å®š output_relativeï¼Œä½† export_markdown=Falseï¼Œç•¥é Markdown åŒ¯å‡ºã€‚')\n",
    "    else:\n",
    "        print('â„¹ï¸ ç•¥é Markdown åŒ¯å‡ºï¼ˆexport_markdown=Falseï¼‰ã€‚')\n",
    "\n",
    "if export_word:\n",
    "    if word_output_path is None:\n",
    "        print('âš ï¸ export_word=Trueï¼Œä½†æœªèƒ½æ±ºå®š Word åŒ¯å‡ºè·¯å¾‘ã€‚')\n",
    "    else:\n",
    "        docx_ready_text = re.sub(r'[ \\t]{2,}$', '', processed, flags=re.MULTILINE)\n",
    "        toc_marker = globals().get('toc_marker', r'\\tableofcontents')\n",
    "        placeholder_used = False\n",
    "        if toc_marker and toc_marker in docx_ready_text:\n",
    "            docx_ready_text = docx_ready_text.replace(toc_marker, TOC_PLACEHOLDER, 1)\n",
    "            placeholder_used = True\n",
    "            docx_ready_text = docx_ready_text.replace(toc_marker, '')\n",
    "        if not placeholder_used:\n",
    "            docx_ready_text = docx_ready_text.replace(TOC_PLACEHOLDER, '')\n",
    "        export_to_docx(docx_ready_text, word_output_path, reference_doc=word_template_path)\n",
    "        post_process_docx(word_output_path)\n",
    "else:\n",
    "    if word_output_override is not None:\n",
    "        print('â„¹ï¸ å·²è¨­å®š word_output_relativeï¼Œä½† export_word=Falseï¼Œç•¥é Word åŒ¯å‡ºã€‚')\n",
    "    else:\n",
    "        print('â„¹ï¸ ç•¥é Word åŒ¯å‡ºï¼ˆexport_word=Falseï¼‰ã€‚')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb2e7a3",
   "metadata": {},
   "source": [
    "## åŸ·è¡Œèˆ‡å¾ŒçºŒè‡ªå‹•åŒ–\n",
    "\n",
    "1. ä¾éœ€è¦èª¿æ•´ä¸Šæ–¹è¨­å®šï¼Œä¾åºåŸ·è¡Œå…©å€‹ç¨‹å¼ç¢¼æ ¼ã€‚\n",
    "2. Notebook æœƒï¼š\n",
    "   - ç¢ºèªè¼¸å‡ºè·¯å¾‘ã€å¥—ç”¨æ–‡å­—è™•ç†ï¼›\n",
    "   - ä»¥ Pandoc ç”¢ç”Ÿ Wordï¼›\n",
    "   - é€é python-docx é‡æ–°æ’å…¥ä¸­æ–‡ç›®éŒ„ã€å¥—ç”¨ç¯„æœ¬æ¨£å¼ï¼Œä¸¦åœ¨éœ€è¦æ™‚æ–°å¢åˆ†é ã€‚\n",
    "3. ç›®éŒ„æ¬„ä½æœƒç”Ÿæˆï¼Œä½†åœ¨ Docker å…§ç„¡æ³•å«ç”¨ Microsoft Wordï¼Œå› æ­¤**æ¬„ä½ä¸æœƒè‡ªå‹•å±•é–‹**ã€‚è«‹åœ¨ä¸»æ©Ÿä¸Šä½¿ç”¨ AppleScript/Automator å°è¼¸å‡ºçš„ `.docx` åŸ·è¡Œã€Œæ›´æ–°æ¬„ä½ã€ã€‚å»ºè­°åšæ³•ï¼š\n",
    "   - å°‡è¼¸å‡ºè³‡æ–™å¤¾æ›è¼‰ç‚º Docker èˆ‡ä¸»æ©Ÿå…±äº«ç›®éŒ„ï¼›\n",
    "   - åœ¨ä¸»æ©Ÿæ’°å¯«ç°¡å–®çš„ç›£æ§è…³æœ¬ï¼ˆä¾‹å¦‚ `fswatch` + AppleScriptï¼‰ï¼Œåµæ¸¬åˆ°æ–°æª”æ¡ˆå¾Œå‘¼å« Word æ›´æ–°æ¬„ä½ï¼›\n",
    "   - å®Œæˆå¾Œå†é–‹å•Ÿ Wordï¼Œå³å¯ç›´æ¥çœ‹åˆ°å®Œæ•´ç›®éŒ„ã€‚\n",
    "\n",
    "> è‹¥å°‡ Notebook ç§»å‡ºå®¹å™¨æˆ–åœ¨å…·å‚™ GUI çš„ç’°å¢ƒåŸ·è¡Œï¼Œå¯åœ¨é€™è£¡è£œä¸Š `osascript` è‡ªå‹•åŒ–æµç¨‹ï¼Œä½†æœ¬å°ˆæ¡ˆé è¨­ä¿ç•™ç‚ºå®¹å™¨å…§ç´”è½‰æª”æµç¨‹ã€‚\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
