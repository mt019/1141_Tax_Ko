{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed1c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ç¢ºèªç’°å¢ƒè®Šæ•¸æ˜¯å¦è®€åˆ°\n",
    "print(\"API Key å­˜åœ¨ï¼Ÿ\", bool(os.getenv(\"OPENAI_API_KEY\")))\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Iterable, Iterator, List\n",
    "\n",
    "# è¨­å®šæ¨¡å‹èˆ‡æª”æ¡ˆè·¯å¾‘ï¼Œè«‹ä¾å¯¦éš›æƒ…æ³æ›´æ–°\n",
    "model = \"gpt-4o-mini\"\n",
    "# model = \"gpt-5-2025-08-07\"\n",
    "client = OpenAI()\n",
    "\n",
    "# å˜—è©¦è‡ªå‹•å®šä½å°ˆæ¡ˆæ ¹ç›®éŒ„\n",
    "possible_roots = [\n",
    "    Path.cwd(),\n",
    "    Path.cwd() / \"work\",\n",
    "    Path.cwd().parent,\n",
    "    Path.cwd().parent / \"work\",\n",
    "]\n",
    "base_dir = None\n",
    "for root in possible_roots:\n",
    "    candidate = root / \"_Material/_å‘½ä»¤/æ•´æ®µå‘½ä»¤.md\"\n",
    "    if candidate.exists():\n",
    "        base_dir = root\n",
    "        break\n",
    "\n",
    "if base_dir is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"åœ¨ä»¥ä¸‹ç›®éŒ„ä¸­æ‰¾ä¸åˆ° _Material/_å‘½ä»¤/æ•´æ®µå‘½ä»¤.mdï¼š{}\".format(\n",
    "            [str(p) for p in possible_roots]\n",
    "        )\n",
    "    )\n",
    "\n",
    "instructions_path = base_dir / \"_Material/_å‘½ä»¤/æ•´æ®µå‘½ä»¤.md\"\n",
    "transcript_path = base_dir / \"mkdocs/My_Notes/èª²_å››34_æ‰€å¾—ç¨æ³•å››/é€å­—ç¨¿/W04_0925.md\"\n",
    "context_paths: List[Path] = [\n",
    "    base_dir / \"_Material/æ³•æº/æ‰€å¾—ç¨…æ³•å››\",\n",
    "    # base_dir / \"mkdocs/My_Notes/èª²_å››34_æ‰€å¾—ç¨æ³•å››\",\n",
    "]\n",
    "output_path = base_dir / \"notebooks/clean_notes_output.md\"\n",
    "\n",
    "# é™åˆ¶åƒæ•¸\n",
    "MAX_CONTEXT_CHARS = 60_000\n",
    "TRANSCRIPT_CHUNK_CHARS = 12_000\n",
    "TRANSCRIPT_CHUNK_OVERLAP = 400\n",
    "\n",
    "instructions = instructions_path.read_text(encoding=\"utf-8\")\n",
    "raw_transcript = transcript_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def load_context_text(path: Path) -> str:\n",
    "    suffix = path.suffix.lower()\n",
    "    if suffix == \".pdf\":\n",
    "        try:\n",
    "            from pypdf import PdfReader\n",
    "        except ImportError as exc:\n",
    "            raise ImportError(\"éœ€è¦å…ˆå®‰è£ pypdfï¼ˆpip install pypdfï¼‰æ‰èƒ½è®€å– PDF è£œå……è³‡æ–™ã€‚\") from exc\n",
    "\n",
    "        reader = PdfReader(str(path))\n",
    "        pages = []\n",
    "        for idx, page in enumerate(reader.pages, start=1):\n",
    "            text = page.extract_text() or \"\"\n",
    "            pages.append(f\"[ç¬¬ {idx} é ]\\n{text.strip()}\")\n",
    "        combined = \"\\n\\n\".join(pages).strip()\n",
    "        if not combined:\n",
    "            raise ValueError(f\"PDF è£œå……è³‡æ–™æœªæ“·å–åˆ°æ–‡å­—ï¼š{path}\")\n",
    "        return combined\n",
    "\n",
    "    try:\n",
    "        return path.read_text(encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "\n",
    "def iter_context_files(paths: Iterable[Path]) -> Iterable[Path]:\n",
    "    allowed_suffixes = {\".md\", \".txt\", \".pdf\"}\n",
    "    for raw_path in paths:\n",
    "        resolved = raw_path if raw_path.is_absolute() else (base_dir / raw_path)\n",
    "        if not resolved.exists():\n",
    "            raise FileNotFoundError(f\"æ‰¾ä¸åˆ°è£œå……è³‡æ–™ï¼š{resolved}\")\n",
    "        # ğŸš« è·³éå«æœ‰ã€Œé€å­—ç¨¿ã€çš„è·¯å¾‘\n",
    "        if \"é€å­—ç¨¿\" in str(resolved):\n",
    "            continue\n",
    "        if resolved.is_file():\n",
    "            if resolved.suffix.lower() in allowed_suffixes:\n",
    "                yield resolved\n",
    "            continue\n",
    "        for child in sorted(resolved.rglob('*')):\n",
    "            # ğŸš« åŒæ¨£æ’é™¤å­è·¯å¾‘å«æœ‰ã€Œé€å­—ç¨¿ã€çš„æƒ…æ³\n",
    "            if \"é€å­—ç¨¿\" in str(child):\n",
    "                continue\n",
    "            if (\n",
    "                child.is_file()\n",
    "                and child.suffix.lower() in allowed_suffixes\n",
    "                and not child.name.startswith('.')\n",
    "            ):\n",
    "                yield child\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    if chunk_size <= 0:\n",
    "        return [text]\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(n, start + chunk_size)\n",
    "        chunks.append(text[start:end])\n",
    "        if end == n:\n",
    "            break\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def estimate_tokens(chars: int) -> int:\n",
    "    return max(1, chars // 4)\n",
    "\n",
    "\n",
    "# --- è™•ç†è£œå……è³‡æ–™ ---\n",
    "context_sections = []\n",
    "context_total_chars = 0\n",
    "skipped_contexts = []\n",
    "for i, ctx_file in enumerate(iter_context_files(context_paths), start=1):\n",
    "    ctx_text = load_context_text(ctx_file).strip()\n",
    "    if not ctx_text:\n",
    "        continue\n",
    "    try:\n",
    "        rel_name = ctx_file.relative_to(base_dir)\n",
    "    except ValueError:\n",
    "        rel_name = ctx_file\n",
    "    section = f\"[è£œå……è³‡æ–™ {i}: {rel_name}]\\n{ctx_text}\"\n",
    "    projected = context_total_chars + len(section)\n",
    "    if projected > MAX_CONTEXT_CHARS:\n",
    "        skipped_contexts.append(str(rel_name))\n",
    "        continue\n",
    "    context_sections.append(section)\n",
    "    context_total_chars = projected\n",
    "\n",
    "context_blob = \"\\n\\n\".join(context_sections)\n",
    "transcript_chunks = chunk_text(raw_transcript, TRANSCRIPT_CHUNK_CHARS, TRANSCRIPT_CHUNK_OVERLAP)\n",
    "if not transcript_chunks:\n",
    "    raise ValueError(\"é€å­—ç¨¿ç©ºç™½æˆ–æœªæˆåŠŸè®€å–å…§å®¹ã€‚\")\n",
    "\n",
    "print(\"å°ˆæ¡ˆæ ¹ç›®éŒ„ï¼š\", base_dir)\n",
    "print(\"ä½¿ç”¨æ¨¡å‹ï¼š\", model)\n",
    "print(\"å·²è¼‰å…¥çš„è£œå……è³‡æ–™æª”æ•¸ï¼š\", len(context_sections))\n",
    "if skipped_contexts:\n",
    "    print(\"âš ï¸ å·²è¶…éè£œå……è³‡æ–™ä¸Šé™ï¼Œæœªç´å…¥çš„æª”æ¡ˆï¼š\")\n",
    "    for name in skipped_contexts:\n",
    "        print(\"   - \", name)\n",
    "print(\"é€å­—ç¨¿åˆ†æ®µæ•¸ï¼š\", len(transcript_chunks))\n",
    "\n",
    "context_tokens = estimate_tokens(len(context_blob))\n",
    "print(\"è£œå……è³‡æ–™ä¼°è¨ˆ tokensï¼š\", context_tokens)\n",
    "\n",
    "chunk_token_estimates = []\n",
    "for idx, chunk_text_value in enumerate(transcript_chunks, start=1):\n",
    "    chunk_tokens = estimate_tokens(len(chunk_text_value))\n",
    "    chunk_token_estimates.append(chunk_tokens)\n",
    "    print(f\"ç¬¬ {idx} æ®µä¼°è¨ˆ tokensï¼š{chunk_tokens} (ç´¯è¨ˆ {sum(chunk_token_estimates)})\")\n",
    "\n",
    "print(\"\\nå¦‚æœç¨‹å¼åŸ·è¡Œåˆ°ä¸€æ®µç´„ 1,000 å­—å°±ä¸­æ–·ï¼Œå¤§æ¦‚å°æ‡‰ \"\n",
    "      f\"{estimate_tokens(1_000)} tokensï¼›åªæœƒè¨ˆå…¥ç•¶å‰è«‹æ±‚å·²é€å‡ºçš„ tokensã€‚\")\n",
    "print(\"è¦å¾—çŸ¥å¯¦éš›æ‰£æ¬¾ï¼Œè«‹åœ¨ API å›å‚³ç‰©ä»¶ä¸­æŸ¥çœ‹ usage.prompt_tokens èˆ‡ usage.completion_tokensã€‚\")\n",
    "\n",
    "\n",
    "# --- é–‹å§‹è™•ç†é€å­—ç¨¿ ---\n",
    "chunk_results: List[str] = []\n",
    "for idx, chunk_text_value in enumerate(transcript_chunks, start=1):\n",
    "    user_parts: List[str] = []\n",
    "    if context_blob:\n",
    "        user_parts.append(context_blob)\n",
    "    user_parts.append(f\"[é€å­—ç¨¿ï¼ˆç¬¬ {idx} æ®µ/å…± {len(transcript_chunks)} æ®µï¼‰]\\n{chunk_text_value}\")\n",
    "    user_content = \"\\n\\n\".join(user_parts)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nâ–¶ï¸ æ­£åœ¨è™•ç†ç¬¬ {idx} æ®µ...\")\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    chunk_pieces: List[str] = []\n",
    "    for chunk in stream:\n",
    "        choice = chunk.choices[0]\n",
    "        delta = getattr(choice, \"delta\", None)\n",
    "        if not delta:\n",
    "            continue\n",
    "        piece = getattr(delta, \"content\", None)\n",
    "        if not piece:\n",
    "            continue\n",
    "        chunk_pieces.append(piece)\n",
    "        print(piece, end=\"\", flush=True)\n",
    "\n",
    "    chunk_output = \"\".join(chunk_pieces)\n",
    "    print(f\"\\n\\nâœ… ç¬¬ {idx} æ®µå®Œæˆ\")\n",
    "    chunk_results.append(chunk_output)\n",
    "\n",
    "print(\"\\nå…¨éƒ¨æ®µè½å·²å®Œæˆï¼Œæ­£åœ¨åŒ¯å‡º...\")\n",
    "final_markdown = \"\\n\\n\".join(chunk_results)\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "output_path.write_text(final_markdown, encoding=\"utf-8\")\n",
    "print(f\"æ•´ç†å¾Œçš„é€å­—ç¨¿å·²å¯«å…¥ï¼š{output_path}\")\n",
    "final_markdown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
