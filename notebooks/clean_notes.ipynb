{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed1c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# 確認環境變數是否讀到\n",
    "print(\"API Key 存在？\", bool(os.getenv(\"OPENAI_API_KEY\")))\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Iterable, Iterator, List\n",
    "\n",
    "# 設定模型與檔案路徑，請依實際情況更新\n",
    "model = \"gpt-4o-mini\"\n",
    "# model = \"gpt-5-2025-08-07\"\n",
    "client = OpenAI()\n",
    "\n",
    "# 嘗試自動定位專案根目錄\n",
    "possible_roots = [\n",
    "    Path.cwd(),\n",
    "    Path.cwd() / \"work\",\n",
    "    Path.cwd().parent,\n",
    "    Path.cwd().parent / \"work\",\n",
    "]\n",
    "base_dir = None\n",
    "for root in possible_roots:\n",
    "    candidate = root / \"_Material/_命令/整段命令.md\"\n",
    "    if candidate.exists():\n",
    "        base_dir = root\n",
    "        break\n",
    "\n",
    "if base_dir is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"在以下目錄中找不到 _Material/_命令/整段命令.md：{}\".format(\n",
    "            [str(p) for p in possible_roots]\n",
    "        )\n",
    "    )\n",
    "\n",
    "instructions_path = base_dir / \"_Material/_命令/整段命令.md\"\n",
    "transcript_path = base_dir / \"mkdocs/My_Notes/課_四34_所得税法四/逐字稿/W04_0925.md\"\n",
    "context_paths: List[Path] = [\n",
    "    base_dir / \"_Material/法源/所得稅法四\",\n",
    "    # base_dir / \"mkdocs/My_Notes/課_四34_所得税法四\",\n",
    "]\n",
    "output_path = base_dir / \"notebooks/clean_notes_output.md\"\n",
    "\n",
    "# 限制參數\n",
    "MAX_CONTEXT_CHARS = 60_000\n",
    "TRANSCRIPT_CHUNK_CHARS = 12_000\n",
    "TRANSCRIPT_CHUNK_OVERLAP = 400\n",
    "\n",
    "instructions = instructions_path.read_text(encoding=\"utf-8\")\n",
    "raw_transcript = transcript_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def load_context_text(path: Path) -> str:\n",
    "    suffix = path.suffix.lower()\n",
    "    if suffix == \".pdf\":\n",
    "        try:\n",
    "            from pypdf import PdfReader\n",
    "        except ImportError as exc:\n",
    "            raise ImportError(\"需要先安裝 pypdf（pip install pypdf）才能讀取 PDF 補充資料。\") from exc\n",
    "\n",
    "        reader = PdfReader(str(path))\n",
    "        pages = []\n",
    "        for idx, page in enumerate(reader.pages, start=1):\n",
    "            text = page.extract_text() or \"\"\n",
    "            pages.append(f\"[第 {idx} 頁]\\n{text.strip()}\")\n",
    "        combined = \"\\n\\n\".join(pages).strip()\n",
    "        if not combined:\n",
    "            raise ValueError(f\"PDF 補充資料未擷取到文字：{path}\")\n",
    "        return combined\n",
    "\n",
    "    try:\n",
    "        return path.read_text(encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "\n",
    "def iter_context_files(paths: Iterable[Path]) -> Iterable[Path]:\n",
    "    allowed_suffixes = {\".md\", \".txt\", \".pdf\"}\n",
    "    for raw_path in paths:\n",
    "        resolved = raw_path if raw_path.is_absolute() else (base_dir / raw_path)\n",
    "        if not resolved.exists():\n",
    "            raise FileNotFoundError(f\"找不到補充資料：{resolved}\")\n",
    "        # 🚫 跳過含有「逐字稿」的路徑\n",
    "        if \"逐字稿\" in str(resolved):\n",
    "            continue\n",
    "        if resolved.is_file():\n",
    "            if resolved.suffix.lower() in allowed_suffixes:\n",
    "                yield resolved\n",
    "            continue\n",
    "        for child in sorted(resolved.rglob('*')):\n",
    "            # 🚫 同樣排除子路徑含有「逐字稿」的情況\n",
    "            if \"逐字稿\" in str(child):\n",
    "                continue\n",
    "            if (\n",
    "                child.is_file()\n",
    "                and child.suffix.lower() in allowed_suffixes\n",
    "                and not child.name.startswith('.')\n",
    "            ):\n",
    "                yield child\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    if chunk_size <= 0:\n",
    "        return [text]\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(n, start + chunk_size)\n",
    "        chunks.append(text[start:end])\n",
    "        if end == n:\n",
    "            break\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def estimate_tokens(chars: int) -> int:\n",
    "    return max(1, chars // 4)\n",
    "\n",
    "\n",
    "# --- 處理補充資料 ---\n",
    "context_sections = []\n",
    "context_total_chars = 0\n",
    "skipped_contexts = []\n",
    "for i, ctx_file in enumerate(iter_context_files(context_paths), start=1):\n",
    "    ctx_text = load_context_text(ctx_file).strip()\n",
    "    if not ctx_text:\n",
    "        continue\n",
    "    try:\n",
    "        rel_name = ctx_file.relative_to(base_dir)\n",
    "    except ValueError:\n",
    "        rel_name = ctx_file\n",
    "    section = f\"[補充資料 {i}: {rel_name}]\\n{ctx_text}\"\n",
    "    projected = context_total_chars + len(section)\n",
    "    if projected > MAX_CONTEXT_CHARS:\n",
    "        skipped_contexts.append(str(rel_name))\n",
    "        continue\n",
    "    context_sections.append(section)\n",
    "    context_total_chars = projected\n",
    "\n",
    "context_blob = \"\\n\\n\".join(context_sections)\n",
    "transcript_chunks = chunk_text(raw_transcript, TRANSCRIPT_CHUNK_CHARS, TRANSCRIPT_CHUNK_OVERLAP)\n",
    "if not transcript_chunks:\n",
    "    raise ValueError(\"逐字稿空白或未成功讀取內容。\")\n",
    "\n",
    "print(\"專案根目錄：\", base_dir)\n",
    "print(\"使用模型：\", model)\n",
    "print(\"已載入的補充資料檔數：\", len(context_sections))\n",
    "if skipped_contexts:\n",
    "    print(\"⚠️ 已超過補充資料上限，未納入的檔案：\")\n",
    "    for name in skipped_contexts:\n",
    "        print(\"   - \", name)\n",
    "print(\"逐字稿分段數：\", len(transcript_chunks))\n",
    "\n",
    "context_tokens = estimate_tokens(len(context_blob))\n",
    "print(\"補充資料估計 tokens：\", context_tokens)\n",
    "\n",
    "chunk_token_estimates = []\n",
    "for idx, chunk_text_value in enumerate(transcript_chunks, start=1):\n",
    "    chunk_tokens = estimate_tokens(len(chunk_text_value))\n",
    "    chunk_token_estimates.append(chunk_tokens)\n",
    "    print(f\"第 {idx} 段估計 tokens：{chunk_tokens} (累計 {sum(chunk_token_estimates)})\")\n",
    "\n",
    "print(\"\\n如果程式執行到一段約 1,000 字就中斷，大概對應 \"\n",
    "      f\"{estimate_tokens(1_000)} tokens；只會計入當前請求已送出的 tokens。\")\n",
    "print(\"要得知實際扣款，請在 API 回傳物件中查看 usage.prompt_tokens 與 usage.completion_tokens。\")\n",
    "\n",
    "\n",
    "# --- 開始處理逐字稿 ---\n",
    "chunk_results: List[str] = []\n",
    "for idx, chunk_text_value in enumerate(transcript_chunks, start=1):\n",
    "    user_parts: List[str] = []\n",
    "    if context_blob:\n",
    "        user_parts.append(context_blob)\n",
    "    user_parts.append(f\"[逐字稿（第 {idx} 段/共 {len(transcript_chunks)} 段）]\\n{chunk_text_value}\")\n",
    "    user_content = \"\\n\\n\".join(user_parts)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n▶️ 正在處理第 {idx} 段...\")\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    chunk_pieces: List[str] = []\n",
    "    for chunk in stream:\n",
    "        choice = chunk.choices[0]\n",
    "        delta = getattr(choice, \"delta\", None)\n",
    "        if not delta:\n",
    "            continue\n",
    "        piece = getattr(delta, \"content\", None)\n",
    "        if not piece:\n",
    "            continue\n",
    "        chunk_pieces.append(piece)\n",
    "        print(piece, end=\"\", flush=True)\n",
    "\n",
    "    chunk_output = \"\".join(chunk_pieces)\n",
    "    print(f\"\\n\\n✅ 第 {idx} 段完成\")\n",
    "    chunk_results.append(chunk_output)\n",
    "\n",
    "print(\"\\n全部段落已完成，正在匯出...\")\n",
    "final_markdown = \"\\n\\n\".join(chunk_results)\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "output_path.write_text(final_markdown, encoding=\"utf-8\")\n",
    "print(f\"整理後的逐字稿已寫入：{output_path}\")\n",
    "final_markdown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
